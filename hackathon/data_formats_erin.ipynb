{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Starting Point for New Data Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on NERSC machine 'cori'\n",
      "  with access to repos: mp107\n",
      "Using default repo mp107\n",
      "Reservation 'toast3' valid from 2019-10-16T09:00:00 to 2019-10-16T17:00:00\n",
      "Current time is 2019-10-16T11:08:10.418371\n",
      "Selecting reservation 'toast3'\n"
     ]
    }
   ],
   "source": [
    "# Are you using a special reservation for a workshop?\n",
    "# If so, set it here:\n",
    "nersc_reservation = \"toast3\"\n",
    "\n",
    "# Load common tools for all lessons\n",
    "import sys\n",
    "sys.path.insert(0, \"../lessons\")\n",
    "from lesson_tools import (\n",
    "    check_nersc,\n",
    ")\n",
    "nersc_host, nersc_repo, nersc_resv = check_nersc(reservation=nersc_reservation)\n",
    "\n",
    "# Capture C++ output in the jupyter cells\n",
    "%reload_ext wurlitzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "f = h5py.File('test.h5')\n",
    "f.create_dataset('channel_one',data=np.ones((10,10)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOD Class\n",
    "\n",
    "This is the stub of a TOD class to read one observation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import toast\n",
    "from toast.mpi import MPI\n",
    "from toast.tod import TOD\n",
    "\n",
    "class NewTOD(TOD):\n",
    "    # You can override the default names of cache keys here.  They\n",
    "    # are defined in the toast.TOD \n",
    "    BORESIGHT_NAME = \"boresight\"\n",
    "    BORESIGHT_AZEL_NAME = \"boresight_azel\"\n",
    "    \"\"\"This class contains the timestream data.\n",
    "\n",
    "    This loads data from a custom data format.  Add more documentation here\n",
    "    about what it is doing...\n",
    "    \n",
    "    Add more constructor arguments to get all the info you need to be\n",
    "    able to read the data.\n",
    "\n",
    "    Args:\n",
    "        path (str):  The path to an observation file.\n",
    "        detquats (dict):  Dictionary of detector names and quaternion\n",
    "            offsets from the boresight.\n",
    "        mpicomm (mpi4py.MPI.Comm): the MPI communicator over which this\n",
    "            observation data is distributed.\n",
    "        detranks (int):  The dimension of the process grid in the detector\n",
    "            direction.  The MPI communicator size must be evenly divisible\n",
    "            by this number.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, path, detquats, mpicomm=None, detranks=1):\n",
    "        self._path = path\n",
    "        self._detquats = detquats\n",
    "        \n",
    "        # Figure out how many samples there are in this observation.  Also,\n",
    "        # if there are any kind of \"sub chunks\" in the observation that should\n",
    "        # not be split up between processes (e.g. left and right azimuth\n",
    "        # scans), then compute them here.\n",
    "        nsamp = 100\n",
    "        \n",
    "        # This is just a list of one element (the whole observation).  You\n",
    "        # could specify the chunks in samples that should never be split up\n",
    "        # between processes.\n",
    "        sampsizes = [nsamp]\n",
    "        \n",
    "        # Here we assign unique IDs to every detector.  This is used for\n",
    "        # reproducible simulations.  You can decide how to assign these for\n",
    "        # your project.  Here they just assigned based on the sorted list\n",
    "        # of detector names.\n",
    "        \n",
    "        detnames = list(sorted(detquats.keys()))\n",
    "        \n",
    "        detindx = {x[1]: x[0] for x in enumerate(detnames)}\n",
    "\n",
    "        # Call base class constructor to distribute data\n",
    "        super().__init__(\n",
    "            mpicomm, detnames, nsamp,\n",
    "            detindx=detindx, detranks=detranks,\n",
    "            sampsizes=sampsizes, meta=dict()\n",
    "        )\n",
    "        \n",
    "        # If we are caching some data (e.g. boresight pointing, auxilliary\n",
    "        # files needed by any read operation, etc) then do it here.  Depending\n",
    "        # on the data format, you may need to just load all data into the\n",
    "        # self.cache object here.\n",
    "        with h5py.File(path,'r') as f:\n",
    "            self.cache.put('signal_channel_one',f['channel_one'])\n",
    "        \n",
    "        #nsamp = len(self.cache.reference('signal_channel_one'))\n",
    "        return\n",
    "\n",
    "    def detoffset(self):\n",
    "        return dict(self._detquats)\n",
    "    \n",
    "    # The methods below assume that the data was cached during construction.\n",
    "    # If not, then you can read the different data products inside each method.\n",
    "    # You can customize the \n",
    "\n",
    "    def _get_boresight(self, start, n):\n",
    "        # This assumes you cached the boresight pointing in RA/DEC\n",
    "        # in the constructor.\n",
    "        ref = self.cache.reference(self.BORESIGHT_NAME)[start:start+n, :]\n",
    "        return ref\n",
    "\n",
    "    def _put_boresight(self, start, data):\n",
    "        ref = self.cache.reference(self.BORESIGHT_NAME)\n",
    "        ref[start:(start+data.shape[0]), :] = data\n",
    "        del ref\n",
    "        return\n",
    "\n",
    "#     def _get_boresight_azel(self, start, n):\n",
    "#         ref = self.cache.reference(self.BORESIGHT_AZEL_NAME)[start:start+n, :]\n",
    "#         return ref\n",
    "\n",
    "#     def _put_boresight_azel(self, start, data):\n",
    "#         ref = self.cache.reference(self.BORESIGHT_AZEL_NAME)\n",
    "#         ref[start:(start+data.shape[0]), :] = data\n",
    "#         del ref\n",
    "#         return\n",
    "\n",
    "    def _get(self, detector, start, n):\n",
    "        name = \"{}_{}\".format(self.SIGNAL_NAME, detector)\n",
    "        ref = self.cache.reference(name)[start:start+n]\n",
    "        return ref\n",
    "\n",
    "    def _put(self, detector, start, data):\n",
    "        name = \"{}_{}\".format(self.SIGNAL_NAME, detector)\n",
    "        ref = self.cache.reference(name)\n",
    "        ref[start:(start+data.shape[0])] = data\n",
    "        del ref\n",
    "        return\n",
    "\n",
    "    def _get_flags(self, detector, start, n):\n",
    "        name = \"{}_{}\".format(self.FLAG_NAME, detector)\n",
    "        ref = self.cache.reference(name)[start:start+n]\n",
    "        return ref\n",
    "\n",
    "    def _put_flags(self, detector, start, flags):\n",
    "        name = \"{}_{}\".format(self.FLAG_NAME, detector)\n",
    "        ref = self.cache.reference(name)\n",
    "        ref[start:(start+flags.shape[0])] = flags\n",
    "        del ref\n",
    "        return\n",
    "\n",
    "    def _get_common_flags(self, start, n):\n",
    "        ref = self.cache.reference(self.COMMON_FLAG_NAME)[start:start+n]\n",
    "        return ref\n",
    "\n",
    "    def _put_common_flags(self, start, flags):\n",
    "        ref = self.cache.reference(self.COMMON_FLAG_NAME)\n",
    "        ref[start:(start+flags.shape[0])] = flags\n",
    "        del ref\n",
    "        return\n",
    "\n",
    "    def _get_hwp_angle(self, start, n):\n",
    "        if self.cache.exists(self.HWP_ANGLE_NAME):\n",
    "            hwpang = self.cache.reference(self.HWP_ANGLE_NAME)[start:start+n]\n",
    "        else:\n",
    "            hwpang = None\n",
    "        return hwpang\n",
    "\n",
    "    def _put_hwp_angle(self, start, hwpang):\n",
    "        ref = self.cache.reference(self.HWP_ANGLE_NAME)\n",
    "        ref[start:(start + hwpang.shape[0])] = hwpang\n",
    "        del ref\n",
    "        return\n",
    "\n",
    "    def _get_times(self, start, n):\n",
    "        ref = self.cache.reference(self.TIMESTAMP_NAME)[start:start+n]\n",
    "        tm = 1.0e-9 * ref.astype(np.float64)\n",
    "        del ref\n",
    "        return tm\n",
    "\n",
    "    def _put_times(self, start, stamps):\n",
    "        ref = self.cache.reference(self.TIMESTAMP_NAME)\n",
    "        ref[start:(start+stamps.shape[0])] = np.array(1.0e9 * stamps,\n",
    "                                                      dtype=np.int64)\n",
    "        del ref\n",
    "        return\n",
    "\n",
    "    def _get_pntg(self, detector, start, n):\n",
    "        # Get boresight pointing (from disk or cache)\n",
    "        bore = self._get_boresight(start, n)\n",
    "        # Apply detector quaternion and return\n",
    "        return qa.mult(bore, self._detquats[detector])\n",
    "\n",
    "    def _put_pntg(self, detector, start, data):\n",
    "        raise RuntimeError(\"This class computes detector pointing on the fly\")\n",
    "        return\n",
    "\n",
    "    def _get_position(self, start, n):\n",
    "        ref = self.cache.reference(self.POSITION_NAME)[start:start+n, :]\n",
    "        return ref\n",
    "\n",
    "    def _put_position(self, start, pos):\n",
    "        ref = self.cache.reference(self.POSITION_NAME)\n",
    "        ref[start:(start+pos.shape[0]), :] = pos\n",
    "        del ref\n",
    "        return\n",
    "\n",
    "    def _get_velocity(self, start, n):\n",
    "        ref = self.cache.reference(self.VELOCITY_NAME)[start:start+n, :]\n",
    "        return ref\n",
    "\n",
    "    def _put_velocity(self, start, vel):\n",
    "        ref = self.cache.reference(self.VELOCITY_NAME)\n",
    "        ref[start:(start+vel.shape[0]), :] = vel\n",
    "        del ref\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NewTOD\n",
       "  1 total detectors and 100 total samples\n",
       "  Using MPI communicator None\n",
       "    In grid dimensions 1 sample ranks x 1 detranks\n",
       "  Process at (0, 0) in grid has data for:\n",
       "    Samples 0 - 99 (inclusive)\n",
       "    Detectors:\n",
       "      channel_one\n",
       "    Cache contains 800 bytes\n",
       ">"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm = toast.Comm()\n",
    "\n",
    "NewTOD('test.h5', detquats={'channel_one':np.array([1,0,0,0])}, mpicomm=comm.comm_group, detranks=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Single Observation\n",
    "\n",
    "This function creates one observation (i.e. a dictionary) with the TOD object and any other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_observation(path, mpicomm=None, detranks=1, **kwargs):\n",
    "    \"\"\"Create an observation.\n",
    "\n",
    "    Extra keyword args are passed to the TOD constructor.\n",
    "\n",
    "    Args:\n",
    "        path (str):  The path to the observation.\n",
    "        mpicomm (mpi4py.MPI.Comm): the MPI communicator over which this\n",
    "            observation data is distributed.\n",
    "        detranks (int):  The dimension of the process grid in the detector\n",
    "            direction.  The MPI communicator size must be evenly divisible\n",
    "            by this number.\n",
    "\n",
    "    Returns:\n",
    "        (dict):  The observation dictionary.\n",
    "\n",
    "    \"\"\"\n",
    "    rank = 0\n",
    "    if mpicomm is not None:\n",
    "        rank = mpicomm.rank\n",
    "\n",
    "    obs = dict()\n",
    "\n",
    "    if rank == 0:\n",
    "        # Rank zero should open up any files to get things needed to construct the TOD\n",
    "        pass\n",
    "\n",
    "    obs[\"tod\"] = NewTOD(path, detquats, mpicomm=mpicomm, detranks=detranks, **kwargs)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Balancing Observations\n",
    "\n",
    "This function computes a \"weight\" for each observation based on the same information that will be given to the TOD constructor.  Here we just return a weight based on the number of samples.  This can be used for an approximate load balancing below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obsweight(path):\n",
    "    \"\"\"Compute observation weight.\n",
    "\n",
    "    Given a path to a \"file\", return the relative weight for this\n",
    "    observation.\n",
    "\n",
    "    Args:\n",
    "        path (str):  Path to the observation\n",
    "\n",
    "    Returns:\n",
    "        (float):  Relative weight\n",
    "\n",
    "    \"\"\"\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Dataset (Multiple Observations)\n",
    "\n",
    "This function takes some parameters and distributes observations among process groups.  Then every group creates their assigned observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toast.dist import distribute_discrete\n",
    "\n",
    "def load_data(dir, obs=None, comm=None, **kwargs):\n",
    "    \"\"\"Loads data.\n",
    "\n",
    "    This should take options for selecting observations based on some criteria.\n",
    "\n",
    "    Additional keyword args are passed to the load_observation function.\n",
    "\n",
    "    Args:\n",
    "        dir (str):  Top directory of data.\n",
    "        obs (list):  The list of observations to load.\n",
    "        comm (toast.Comm): the toast Comm class for distributing the data.\n",
    "\n",
    "    Returns:\n",
    "        (toast.Data):  The distributed data object.\n",
    "\n",
    "    \"\"\"\n",
    "    # the global communicator\n",
    "    cworld = comm.comm_world\n",
    "    # the communicator within the group\n",
    "    cgroup = comm.comm_group\n",
    "\n",
    "    # One process gets the list of observation directories\n",
    "    obslist = list()\n",
    "    weight = dict()\n",
    "\n",
    "    worldrank = 0\n",
    "    if cworld is not None:\n",
    "        worldrank = cworld.rank\n",
    "\n",
    "    if worldrank == 0:\n",
    "#         for root, dirs, files in os.walk(dir, topdown=True):\n",
    "#             for d in dirs:\n",
    "#                 # Get a list of directory names as the \"observations\".  What you\n",
    "#                 # do here depends on how your data is organized.\n",
    "#                 obslist.append(d)\n",
    "#                 weight[d] = obsweight(os.path.join(root, dir))\n",
    "#             break\n",
    "        obslist = [\"foo\", \"bar\", \"blat\", \"obs_to_cut\"]\n",
    "        obslist = sorted(obslist)\n",
    "        # Filter by the requested obs\n",
    "        fobs = list()\n",
    "        if obs is not None:\n",
    "            for ob in obslist:\n",
    "                if ob in obs:\n",
    "                    fobs.append(ob)\n",
    "            obslist = fobs\n",
    "\n",
    "    # Communicate what observations we are using.\n",
    "    if cworld is not None:\n",
    "        obslist = cworld.bcast(obslist, root=0)\n",
    "        weight = cworld.bcast(weight, root=0)\n",
    "\n",
    "    # Distribute observations based on the relative weight.\n",
    "    dweight = [weight[x] for x in obslist]\n",
    "    distobs = distribute_discrete(dweight, comm.ngroups)\n",
    "\n",
    "    # Distributed data\n",
    "    data = Data(comm)\n",
    "\n",
    "    # Now every group adds its observations to the list\n",
    "\n",
    "    firstobs = distobs[comm.group][0]\n",
    "    nobs = distobs[comm.group][1]\n",
    "    for ob in range(firstobs, firstobs+nobs):\n",
    "        opath = os.path.join(dir, obslist[ob])\n",
    "        print(\"Loading {}\".format(opath))\n",
    "        # In case something goes wrong on one process, make sure the job\n",
    "        # is killed.\n",
    "        try:\n",
    "            data.obs.append(\n",
    "                load_observation(opath, mpicomm=cgroup, **kwargs)\n",
    "            )\n",
    "        except:\n",
    "            exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "            lines = traceback.format_exception(exc_type, exc_value,\n",
    "                                               exc_traceback)\n",
    "            lines = [\"Proc {}: {}\".format(worldrank, x)\n",
    "                     for x in lines]\n",
    "            print(\"\".join(lines), flush=True)\n",
    "            if cworld is not None:\n",
    "                cworld.Abort()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this when writing a file for MPI\n",
    "# %%writefile data_formats_mpi.py\n",
    "\n",
    "import toast\n",
    "from toast.mpi import MPI\n",
    "\n",
    "comm = toast.Comm()\n",
    "\n",
    "data = load_data(\"data/directory\", obs=[\"foo\", \"bar\", \"blat\"], comm=comm)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "\n",
    "command = \"python data_formats_mpi.py\"\n",
    "runstr = None\n",
    "\n",
    "if nersc_host is not None:\n",
    "    runstr = \"export OMP_NUM_THREADS=4; srun -N 2 -C haswell -n 32 -c 4 --cpu_bind=cores -t 00:05:00\"\n",
    "    if nersc_resv is not None:\n",
    "        runstr = \"{} --reservation {}\".format(runstr, nersc_resv)\n",
    "else:\n",
    "    # Just use mpirun\n",
    "    runstr = \"mpirun -np 4\"\n",
    "\n",
    "runcom = \"{} {}\".format(runstr, command)\n",
    "print(runcom, flush=True)\n",
    "\n",
    "# Uncomment this line to actually submit the job\n",
    "# sp.check_call(runcom, stderr=sp.STDOUT, shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMB 20191012",
   "language": "python",
   "name": "cmbenv-20191012"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
